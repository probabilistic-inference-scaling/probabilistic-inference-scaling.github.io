<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ConceptFusion: Open-set Multimodal 3D Mapping">
  <meta name="keywords" content="ConceptFusion, 3D Mapping, SLAM, Open-set, Multimodal, Foundation models, CLIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ConceptFusion: Open-set Multimodal 3D Mapping</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JX0F75QDW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>




<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods</h1>
          <h2 class="title is-6 publlication-title">-----</h2>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="ishapuri.github.io">Isha Puri</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=O71amfMAAAAJ&hl=en&oi=ao">Shivchander Sudalairaj</a><sup>*2</sup>,</span>
            <span class="author-block">
              <a href="https://gxxu-ml.github.io/">GX Xu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://xuk.ai/">Kai Xu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://akashgit.github.io/">Akash Srivastava</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>MIT,</span>
            <span class="author-block"><sup>2</sup>Red Hat AI Innovation, MIT-IBM Watson AI Lab</span>
          </div>

          <div class="is-size-7 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <span class="link-block">
                <!-- BELOW SHOULD BE ARXIV LINK -->
                <a href="/Users/ishapuri/Desktop/probabilistic-inference-scaling.github.io/assets/pdf/probabilistic-inference-scaling-paper.pdf"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ishapuri/probabilistic_inference_scaling"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating scaling the computation spent at inference time. Existing inference-time scaling methods, usually with reward models, cast the task as a search problem, which tends to be vulnerable to reward hacking as a consequence of approximation errors in reward models. In this paper, we instead cast inference-time scaling as a probabilistic inference task and leverage sampling-based techniques to explore the typical set of the state distribution of a state-space model with an approximate likelihood, rather than optimize for its mode directly. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods to this task. Our empirical evaluation demonstrates that our methods have a 4--16x better scaling rate over our deterministic search counterparts on various challenging mathematical reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct can surpass GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects the rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Approach</h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">Construct pixel-aligned features</h3>
        <div class="content has-text-justified">
          <p>
            ConceptFusion constructs pixel-aligned features from off-the-shelf foundation models that can only produce a global (image-level) embedding vector. This is achieved by: processing input images to generate generic (class-agnostic) object masks and extracting a local features for each, computing a global feature for the input image as a whole, and fusing the region-specific features with global features using our proposed zero-shot pixel alignment technique.
          </p>
          <img src="./static/images/pipeline.png" />
        </div>
        <br/>
      </div>
    </div>


    <div class="columns is-centered">

      <!-- Zero-shot pixel alignment -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Zero-shot pixel alignment</h2>
          <div class="interpolation-image-wrapper-zero-shot">
            <img src="./static/images/pixel-aligned-feature-computation.png" />
          </div>
          <p>
            For each image, the global (f<sub>G</sub>) and local (f<sub>L</sub>) features are fused to obtain our pixel-aligned features (f<sub>P</sub>). <i>Top-left</i>: We first compute cosine similarities between each local feature (f<sub>L</sub>) with the global feature (f<sub>G</sub>). <i>Top-right</i>: We compute an inter-feature similarity matrix, and compute the average similarity of each local feature to every other local feature, denoted φ̄<sub>i</sub> . <i>Bottom-left</i>: We combine these similarities to produce weights for fusing f<sub>G</sub> and f<sub>L</sub> to obtain pixel-aligned features f<sub>P</sub>.
          </p>
        </div>
      </div>
      <!--/ Zero-shot pixel alignment -->

      <!-- Long-tailed concepts -->
      <div class="column">
        <h2 class="title is-3">Retaining fine-grained concepts</h2>
        <div class="columns is-centered">
          <div class="column content">
            <div class="interpolation-image-wrapper-fine-grained">
              <img src="./static/images/pixel-aligned-qualitative.png" />
            </div>
            <p>
              Our approach to computing pixel-aligned features is adept at capturing long-tailed and fine-grained concepts. The plots to the right show the similarity scores between the embeddings of the cropped image regions corresponding to diet coke, lysol, and yogurt and their text embeddings, predicted by the base CLIP model used by LSeg and OpenSeg respectively. This implies that the base CLIP models know these concepts, yet, as can be seen on the tiled plots (center), LSeg and OpenSeg are not able to retrieve these concepts; they forget the concepts when finetuned. On the other hand, our zero-shot pixel-alignment approach does not suffer this drawback, and clearly delineates the corresponding pixels.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ Long-tailed concepts -->


    <div class="columns is-centered">

      <!-- UnCoCo dataset -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">UnCoCo dataset</h2>
          <div class="interpolation-image-wrapper-uncoco">
            <img src="./static/images/uncoco.png" />
          </div>
          <p>
            To evaluate long-tailed reasoning and multimodal reasoning abilities, since there is no existing dataset, we capture a set of 20 RGB-D sequences comprising 78 commonly found objects and annotate them with text, audio, click, and image queries. For each query, we also provide the corresponding ground truth 2D and 3D retrieval results. This image showcases sample tabletop scenes from UnCoCo (left) and the resulting 3D reconstructions and labels (right).
          </p>
        </div>
      </div>
      <!--/ UnCoCo dataset -->

      <!-- 3D spatial reasoning -->
      <div class="column">
        <h2 class="title is-3">3D spatial reasoning</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/spatial-query.png" />
            <p>
              <i>What is the distance between the refrigerator from the television?</i>
            </p>
            <p>
              A key benefit of lifting foundation features to 3D is the ability to reason about spatial attributes. We implement a set of generic spatial relationship comparators that can be leveraged for querying arbitrary objects. We employ a large language model to parse the queries to generate function calls that can directly be executed. E.g., the query above parses to <tt>howFar(refrigirator, television)</tt>.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ 3D spatial reasoning -->

    <!-- Long-form text queries -->    
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Interpolating. -->
        <h2 class="title is-4">Long-form text queries</h2>
        <div class="content has-text-justified">
          <p>
            ConceptFusion is able to handle long-form text queries and accurately localize objects referenced by the query. In the first two scenarios, OpenSeg is distracted by the presence of several confounding attributes. The third scenario shows a single world query (television) that is part of the COCO Captions dataset used to train OpenSeg, providing it an unfair advantage. ConceptFusion, nonetheless, accurately assigns the highest response to the map points representing the television. In each query, the referenced object is boldfaced.
          </p>
          <img src="./static/images/scannet-text-query.png" />
        </div>
        <br/>
      </div>
    </div>
    <!--/ Long-form text queries -->

    <!-- Click-query video -->    
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-4">Click-queries</h2>
        <div class="content has-text-justified">
          <p>
            Click-queries over a sequence from the ICL dataset. For each clicked point, we compute the cosine similarity of the embedding at that point with that of every other map point and visualize them using a 'jet' colormap. Points in red indicate greatest similarity, while points in blue indicate least similarity. Notice the consistency in semantic concepts. For instance, when we click on a point on the corner lamp (at about 0:45), we also notice that the other corner lamp, as well as lights on top of the ceiling get high similarities assigned.
          </p>
          <video id="click-query-icl" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/ICL Click query cropped.mp4"
                    type="video/mp4">
          </video>
        </div>
        <br/>
      </div>
    </div>
    <!--/ Click-query video -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Experiments on real robotic systems</h2>
    <div class="columns is-centered">

      <!-- Tabletop rearrangement -->
      <div class="column">
        <div class="content">
          <h3 class="title is-3">Tabletop rearrangement</h3>
          <div class="interpolation-image-wrapper-tabletop">
            <img src="./static/images/rearrangement.png" />
          </div>
          <p>
            The robot is provided with rearrangment goals involving novel objects. (Top row) push goldfish to the right of the yellow line, where goldfish refers to the brandname of the pack of Cheddar snack. (Bottom row) push baymax to the right of the yellow line, where baymax refers to the plush toy depicting the famous Disney character.
          </p>
        </div>
      </div>
      <!--/ Tabletop rearrangement -->

      <!-- Self-driving -->
      <div class="column">
        <h3 class="title is-3">Autonomous driving</h3>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/driving.png" />
            <p>
              (Left to right; top to bottom) Autonomous drive-by-wire platform deployed; pointcloud map of the environment with the response to the openset text-query ”football field” (shown in red); path found to the football field (shown in red); car successfully navigates to the destination autonomously. See our anonymized webpage for more results.
            </p>
          </div>

        </div>
      </div>
    </div>
    <!--/ Self-driving -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Integrating ConceptFusion with Large Language Models</h2>
    <div class="columns is-centered">

      <!-- GPT video 1 -->
      <div class="column">
        <div class="content">
          <video id="gpt-video-1" autoplay controls muted playsinline height="100%">
            <source src="./static/videos/gpt video.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ GPT video 1 -->

      <!-- GPT video 2 -->
      <div class="column">
        <div class="content">
          <video id="gpt-video-2" autoplay controls muted playsinline height="100%">
            <source src="./static/videos/gpt video2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
      <!--/ GPT video 2 -->
    </div>
  </div>
</section>


<section class="section" id="concurrent work">
  <div class="container is-max-desktop content">
    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Concurrent work</h2>

        <div class="content has-text-justified">
          <p>
            Given the pace of AI research these days, it is extremely challenging to keep up with all of the work around foundation models and open-set perception. We list below a few key approaches that we have come across after beginning work on ConceptFusion. If we may have inadvertently missed out on key concurrent work, please reach out to us over email (or better, open a pull request on <a href="https://github.com/concept-fusion/concept-fusion.github.io">our GitHub page</a>).
          </p>
          <p>
            <a href="https://mahis.life/clip-fields/">CLIP-Fields</a> encodes features from language and vision-language models into a compact, scene-specific neural network trained to predict feature embeddings from 3D point coordinates; to enable open-set visual understanding tasks.
          </p>
          <p>
            <a href="https://pengsongyou.github.io/openscene">OpenScene</a> demonstrates that features from pixel-aligned 2D vision-language models can be distilled to 3D, generalize to new scenes, and perform better than their 2D counterparts.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2211.16312">Deng et al.</a> demonstrate interesting ways of learning hierarchical scene abstractions by distilling features from 2D vision-language foundation models, and smart ways of interpreting captions from 2D captioning approaches.
          </p>
          <p>
            <a href="https://makezur.github.io/FeatureRealisticFusion/">Feature-realistic neural fusion</a> demonstrates the integration of DINO features into a real-time neural mapping and positioning system.
          </p>
          <p>
            <a href="https://semantic-abstraction.cs.columbia.edu">Semantic Abstraction</a> uses CLIP features to generate 3D features for reasoning over long-tailed categories, for scene completion and detecting occluded objects from language.
          <p>
            <a href="https://say-can.github.io/">Say-Can</a> demonstrates the applicability of large language models as task-planners, and leverage a set of low-level skills to execute these plans in the real world. Also related to this line of work are <a href="https://vlmaps.github.io/">VL-Maps</a>, <a href="https://nlmap-saycan.github.io/">NLMap-SayCan</a>, and <a href="https://cow.cs.columbia.edu/">CoWs</a>, which demonstrate the benefits of having a map queryable via language.
          </p>
          <p>
            <a href="lerf.io">Language embedded radiance fields (LERF)</a> trains a NeRF that additionally encodes CLIP and DINO features for language-based concept retrieval.
          </p>
          <p>
            <a href="https://vis-www.cs.umass.edu/3d-clr/">3D concept learning from multi-view images (3D-CLR)</a> introduces a dataset for 3D multi-view visual question answering, and proposes a concept learning framework that leverages pixel-aligned language embeddings from LSeg. They additionally train a set of neurosymbolic reasoning modules that loosely inspire our spatial query modules.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<script type="text/javascript">
  $(function() {
  var screenWidth = $(window).width();
  if (screenWidth >= 800) {
    $('#gpt-video-1').attr('autoplay', 'autoplay');
  }
  if (screenWidth >= 800) {
    $('#gpt-video-2').attr('autoplay', 'autoplay');
  }
  if (screenWidth >= 800) {
    $('#click-query-icl').attr('autoplay', 'autoplay');
  }
});
</script>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{probabilistic-inference-scaling,
  author    = {Puri, Isha and Sudalairaj, Shivchander and Xu, Guangxuan and Xu, Kai and Srivastava, Akash},
  title     = {A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods 	},
  year      = {2025},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./assets/pdf/probabilistic-inference-scaling-paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/concept-fusion" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
        <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website adapted from the Nerfies templates, which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Source Code: <a href="https://github.com/nerfies/nerfies.github.io">Nerfies source code</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
